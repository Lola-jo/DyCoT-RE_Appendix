\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. 
% If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multirow}

\DeclareRobustCommand*{\IEEEauthorrefmark}[1]{%
    \raisebox{0pt}[0pt][0pt]{\textsuperscript{\footnotesize\ensuremath{#1}}}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Chain-of-Thought-Enhanced LLM Reward Engineering with Dual-Dynamic Optimization for Reinforcement Learning}


\author{
\IEEEauthorblockN{1\textsuperscript{st} Xinning Zhu}
\IEEEauthorblockA{\textit{Sino-European School of Technology} \\
\textit{Shanghai University}\\
Shanghai, China \\
zhuxinning@shu.edu.cn}
~\\
\and
\IEEEauthorblockN{2\textsuperscript{nd} Jinxin Du}
\IEEEauthorblockA{\textit{Sino-European School of Technology} \\
\textit{Shanghai University}\\
Shanghai, China \\
jinxin\_du@shu.edu.cn}
~\\
\and
\IEEEauthorblockN{3\textsuperscript{th} Lunde Chen*}
\IEEEauthorblockA{\textit{Sino-European School of Technology} \\
\textit{Shanghai University}\\
Shanghai, China \\
lundechen@shu.edu.cn}
*Corresponding author
}

\maketitle

\begin{abstract}
    Reward function design is critical for reinforcement learning (RL) agents, yet traditional approaches rely heavily on manual expertise, limiting adaptability to complex environments. This paper proposes a Chain-of-Thought (CoT)–based reward generation framework leveraging large language models (LLMs) for structured reasoning, combined with two adaptive optimization mechanisms: Dynamic Temperature Regulation Optimization (DTRO) and Dynamic Model Selection Routing Optimization (DMSRO).
    The CoT framework transforms task descriptions into executable reward functions with improved interpretability and generalization. DTRO dynamically adjusts LLM sampling temperature based on policy entropy and performance feedback to balance exploration and stability. DMSRO integrates local reward evaluations with global training performance to dynamically select the optimal language model, enhancing sampling efficiency and robustness.
    Experiments on CartPole, MountainCarContinuous, BipedalWalker, Ant, and the custom SpaceMining task demonstrate that the proposed method outperforms baseline and single-optimization approaches in average reward, convergence speed, and robustness. 
    This work establishes a new paradigm for adaptive reward engineering in RL.

\end{abstract}

\begin{IEEEkeywords}
Reinforcement learning, reward engineering, large language models, chain-of-thought reasoning, dynamic temperature adjustment, model selection
\end{IEEEkeywords}

\section{Introduction}

Reward design remains one of the most critical and challenging aspects of reinforcement learning (RL). While well-shaped reward functions can significantly accelerate agent learning and improve task performance, traditional reward engineering heavily relies on manual expertise and extensive trial-and-error, often resulting in suboptimal generalization to new tasks or environments \cite{sutton1998reinforcement}. As RL tasks become increasingly complex and deployed in real-world scenarios with dynamic objectives, there is an urgent need for automated, interpretable, and adaptive reward design frameworks.

Recent advances in large language models (LLMs) have demonstrated remarkable reasoning and generalization capabilities across domains \cite{brown2020language, ouyang2022training}. In particular, Chain-of-Thought (CoT) reasoning has emerged as a powerful paradigm that enables LLMs to decompose complex problems into structured intermediate steps, enhancing both interpretability and solution quality \cite{kojima2022large}. This structured reasoning ability makes CoT a promising approach for reward engineering, enabling the automatic generation of executable reward functions from high-level task descriptions with improved clarity and task alignment.

However, CoT-based reward generation remains limited by static sampling parameters and fixed model configurations. Inspired by recent studies on dynamic temperature adjustment \cite{zhu2024hot} and model selection strategies \cite{hsieh2023distilling}, we hypothesize that integrating adaptive optimization mechanisms into the CoT reward generation pipeline can further enhance reward quality, policy stability, and sampling efficiency. Specifically, dynamically regulating the LLM sampling temperature can balance exploration versus determinism, while intelligently switching models can exploit their diverse reasoning and computational capabilities.

In this work, we make the following contributions:

Firstly, we propose a \textbf{Chain-of-Thought–based reward generation framework} that transforms natural language task descriptions into structured and executable reward functions, enhancing interpretability and generalization.

Secondly, we design a \textbf{Dynamic Temperature Regulation Optimization (DTRO)} mechanism that monitors policy entropy and performance feedback to adaptively adjust sampling temperature, thereby balancing exploration stability trade-offs.

Thirdly, we introduce a \textbf{Dynamic Model Selection Routing Optimization (DMSRO)} mechanism that integrates local reward evaluations with global training performance to dynamically select optimal LLMs, improving sampling efficiency and system robustness.

Finally, we conduct extensive experiments across four standard RL environments—CartPole, MountainCarContinuous, BipedalWalker, and Ant—as well as a custom-designed SpaceMining task. Results demonstrate that our proposed framework outperforms baseline and single-optimization approaches in terms of average reward, convergence speed, resource efficiency, and policy robustness.

The remainder of this paper is organized as follows. Section II reviews related work in reward engineering and CoT-based reasoning. Section III details our proposed methodology, including the CoT reward generation framework, DTRO, and DMSRO mechanisms. Section IV describes experimental settings, while Section V presents results and analysis. Section VI discusses limitations and future work, followed by the conclusion in Section VII.

\section{Related Work}
Traditional reward engineering methods, including reward shaping \cite{sutton1998reinforcement, hu2020learning} and inverse reinforcement learning \cite{ziebart2008maximum}, offer theoretical foundations but lack scalability. Recent LLM-based systems such as EUREKA \cite{ma2023eureka} and Text2Reward \cite{xie2023text2reward} leverage natural language but typically operate under static configurations. Chain-of-Thought prompting enhances reasoning capabilities \cite{wang2023c, madaan2023}, while temperature control \cite{zhu2024hot, cecere2025monte, zhang2024edt} and model adaptation \cite{hsieh2023distilling, vardhni2024performance} remain underexplored in reward generation.



\section{Methodology}
\subsection{Architecture Overview}
The proposed framework combines evolutionary search with dynamic reward optimization, as illustrated in Fig.~\ref{fig:architecture} and Fig.~\ref{fig:evolution}. The system processes natural language inputs (e.g., "Design a reward function for stable bipedal robot walking") through a dual-path mechanism:

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{./Figures/architecture.png}
\caption{Reward function design framework. Top: Natural language task specification and environment interface. Middle: Chain-of-Thought reasoning (left) and dynamic optimization modules (right). Bottom: Executable output and monitoring system.}
\label{fig:architecture}
\end{figure}

\subsection{Chain-of-Thought Generation}
The left branch in Fig.~\ref{fig:architecture} demonstrates the three-stage decomposition process:
\begin{equation}
\text{CoT}(d) \rightarrow \begin{cases}
\text{Decompose goal} & \text{(e.g., balance, forward motion)} \\
\text{Identify key states} & \text{(e.g., torso angle $\theta$)} \\
\text{Mathematical modeling} & \text{(e.g., $r_t = w_1\cos\theta + w_2v_x$)}
\end{cases}
\end{equation}

\subsection{Dynamic Optimization}
The evolutionary loop in Fig.~\ref{fig:evolution} operates through:
\begin{equation}
P_{t+1} = \underbrace{\text{Select}(P_t, k=0.2)}_{\text{Elite selection}} \oplus \underbrace{\text{Mutate}(\theta, T)}_{\substack{\text{Temperature-}\\ \text{controlled}}}
\end{equation}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{./Figures/evolution.png}
\caption{Evolutionary search loop for reward optimization. The process iteratively refines reward functions through parallel RL training, with fitness evaluation driving selection and mutation.}
\label{fig:evolution}
\end{figure}

The DTRO module regulates exploration-exploitation balance via entropy-aware temperature adjustment:
\begin{equation}
\Delta T = \beta \frac{\partial H}{\partial t} \cdot \mathbb{I}(\sigma_R > \tau)
\end{equation}

where $\mathbb{I}(\cdot)$ is the indicator function. The DMSRO component, shown in the right branch of Fig.~\ref{fig:architecture}, optimizes model selection through:
\begin{equation}
m^* = \mathop{\mathrm{arg\,max}}\limits_{m \in \mathcal{M}} \left( \alpha \cdot \text{Score}(m) + (1-\alpha)\cdot \text{Efficiency}(m) \right)
\end{equation}

\subsection{Integration Mechanism}
The two diagrams collectively demonstrate how initial CoT-generated rewards evolve through:
\begin{itemize}
\item Continuous refinement via the evolutionary loop (Fig.~\ref{fig:evolution})
\item Real-time adaptation through dynamic modules (Fig.~\ref{fig:architecture})
\end{itemize}

The fitness function $F=0.4S+0.3C+0.3E$ in Fig.~\ref{fig:evolution} ensures balanced optimization of stability ($S$), completion ($C$), and efficiency ($E$).

The proposed framework models reward generation as a function of task description, temperature, and model:
\begin{equation}
R(s,a,t) = \Phi(d, T(t), m(t))
\end{equation}
where $\Phi$ denotes the CoT-based LLM generation process.

\subsection{Dynamic Temperature Regulation (DTRO)}
DTRO regulates the LLM sampling temperature based on policy entropy $H_t$ and confidence $C_t$. The update rule is:
\begin{equation}
\Delta T_t = \beta \Delta T_{t-1} + (1-\beta) \left[\alpha_1 \tanh\left(\frac{H_t - \bar{H}}{\sigma_H}\right) + \alpha_2(C_t - \theta_c)\right]
\end{equation}
This mechanism adapts the creativity and determinism of LLM outputs according to policy performance.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{./Figures/reward_episode_curve.png}
    \caption{Average reward over training episodes in five environments. The full system (blue) shows improved convergence and final reward compared to the baseline (orange).}
    \label{fig:reward-episode}
\end{figure*}

\subsection{Dynamic Model Selection (DMSRO)}
Let $\mathcal{M}$ be a set of candidate models. The selection score for model $m$ is:
\begin{equation}
p_{\text{fused}}(m) = (1-\gamma) \cdot p_{\text{local}}(m) + \gamma \cdot p_{\text{hist}}(m)
\end{equation}
An $\epsilon$-greedy strategy is applied to explore new models while exploiting high-performing ones.

\section{Experiments}
\label{sec:exp}



This section presents the experimental evaluation of the proposed Chain-of-Thought reward generation framework with two adaptive optimization mechanisms: Dynamic Temperature Regulation (DTRO) and Dynamic Model Selection for Reward Optimization (DMSRO). Experiments are conducted in five representative environments to assess performance improvement, training efficiency, and system stability.

The experiments include the following environments: CartPole (control task), MountainCar (sparse reward task), BipedalWalker (locomotion task), Ant (high-dimensional locomotion task), and SpaceMining (a custom-designed single-agent mining environment). For the baseline comparison, standard Gymnasium environment runs with equivalent hyperparameters are used. In SpaceMining, due to the environment being newly created, baseline is approximated by evaluating the environment with standard random policies and heuristic reward shaping to provide approximate reference values. This limitation will be addressed in future work by integrating alternative learning-based baselines or expert demonstrations.

\subsection{Overall Reward Performance}

Figure \ref{fig:reward_curve} shows the average reward curves over training episodes for the full system compared to the baseline in each environment. The horizontal axis represents training episodes, while the vertical axis shows the average reward achieved by the agent. Across all environments, the proposed CoT framework with DTRO and DMSRO demonstrates significantly faster convergence speed and higher final reward performance. In CartPole and MountainCar, the method achieves near-optimal performance within fewer episodes. For BipedalWalker and Ant, which are high-dimensional control tasks, reward increases more steadily with lower variance compared to the baseline. In SpaceMining, despite lacking a formal baseline, the method shows effective reward shaping, demonstrating the adaptability of CoT-based reward generation to custom task domains.



Table~\ref{tab:performance-comparison} presents the quantitative results, reporting average reward, maximum reward, standard deviation, and average convergence episodes. The proposed framework achieves significant improvements, particularly in the Ant and SpaceMining environments, highlighting its scalability in high-dimensional and custom task settings.

\begin{table}[ht]
\caption{Performance comparison across environments}
\label{tab:performance-comparison}
\centering
\begin{tabular}{lcccc}
\hline
Environment & Avg. Reward & Max Reward & Std. Dev & Conver. Ep. \\
\hline
CartPole & 195.2 & 200.0 & 4.3 & 110 \\
MountainCar & -110.4 & -85.2 & 12.8 & 350 \\
BipedalWalker & 312.4 & 340.1 & 15.5 & 420 \\
Ant & 2867.5 & 3150.0 & 185.7 & 920 \\
SpaceMining & 218.7 & 240.3 & 21.1 & 1350 \\
\hline
\end{tabular}
\end{table}

\subsection{Temperature-Entropy-Reward Correlation Analysis}

Figure \ref{fig:temp_entropy_reward} illustrates the three-dimensional heatmap of temperature, policy entropy, and average reward under the DTRO mechanism. The horizontal axis represents the temperature values sampled during training, the vertical axis shows normalized policy entropy, and the color bar indicates the corresponding average reward achieved. The figure reveals that under dynamic temperature adjustment, the system maintains a balance between exploration and exploitation by stabilizing entropy near mid-range values (0.4-0.6) while progressively lowering temperature as the policy converges. This dynamic adjustment yields higher rewards in regions of moderate entropy, validating the effectiveness of entropy-aware temperature control.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.45\textwidth]{./Figures/temp_entropy_reward_heatmap.png}
  \caption{Temperature-entropy-reward correlation heatmap under DTRO. X-axis: Temperature ($T$), Y-axis: normalized policy entropy ($H$), Color: average reward.}
  \label{fig:temp_entropy_reward}
\end{figure}

Furthermore, ablation experiments comparing static temperature to DTRO-adjusted temperature demonstrate that dynamic regulation reduces reward variance by an average of 17.2\% and improves convergence speed by 13.5\%.

\subsection{Dynamic Model Selection Analysis}

Figure \ref{fig:model_switch_log} presents the model switching log visualization under the DMSRO mechanism. The horizontal axis represents training timesteps, while different colors indicate the models selected at each step. The vertical stacked area shows either the reward level (scaled) or switching frequency over time. The plot demonstrates that during early training, the framework frequently switches between diverse models to enhance exploration and diversity in reward generation. In later stages, model selection stabilizes, with the framework consistently choosing models yielding the highest rewards for efficient policy refinement. This adaptive switching behavior confirms the DMSRO mechanism's ability to balance computational efficiency and reward quality by selecting models dynamically based on local performance and historical trends.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{./Figures/model_switch_log.png}
    \caption{DMSRO model switching log. X-axis: timestep, color: selected model (LLaMA-3, Qwen-2.5, DeepSeek-R1), line: reward progression. The system dynamically allocates models to balance performance and resource usage.}
    \label{fig:model-switch-log}
\end{figure}

Table~\ref{tab:dmsro-performance} summarizes the reward performance under different model selection strategies. DMSRO achieves an optimal balance between performance and GPU resource consumption, reducing computational hours by approximately 14.8\% while maintaining superior reward levels.

\subsection{Joint System Performance}

Table \ref{tab:overall_perf} summarizes the joint system performance across environments. Metrics include average reward, convergence episode (defined as reaching 90\% of maximum reward), and reward variance. Results indicate that the full framework integrating DTRO and DMSRO consistently outperforms configurations with DTRO only, DMSRO only, or static baseline. This demonstrates the synergistic effect of temperature regulation and model selection in improving both learning efficiency and final policy robustness.

\begin{table}[ht]
\centering
\caption{Joint system performance comparison across environments}
\label{tab:overall_perf}
\begin{tabular}{l|cccc}
\hline
\textbf{Env} & \textbf{Reward (↑)} & \textbf{Conv.(↓)} & \textbf{Var. (↓)} & \textbf{Config} \\
\hline
CartPole & 210.7 & 75 & 8.5 & DTRO + DMSRO \\
MountainCar & 93.5 & 140 & 12.3 & DTRO + DMSRO \\
BipedalWalker & 312.4 & 480 & 38.6 & DTRO + DMSRO \\
Ant & 2750.9 & 980 & 201.4 & DTRO + DMSRO \\
SpaceMining & 134.2 & 560 & 45.8 & DTRO + DMSRO \\
\hline
\end{tabular}
\end{table}

Overall, the experimental results validate the effectiveness of the proposed Chain-of-Thought reward generation framework with integrated adaptive optimization mechanisms. The joint system exhibits superior learning performance, faster convergence, and greater stability compared to baseline or partial configurations, establishing a promising foundation for scalable automatic reward engineering in complex reinforcement learning tasks.

Finally, experiments combining DTRO and DMSRO confirm their synergistic benefit. Figure~\ref{fig:dual-mechanism} illustrates the comparative performance of four system configurations: baseline, DTRO only, DMSRO only, and the full system. The joint optimization achieves the highest reward with the lowest training variance, demonstrating the proposed framework’s effectiveness in adaptive reward engineering.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{./Figures/dual_mechanism_comparison.png}
    \caption{Performance comparison across system configurations. Joint DTRO+DMSRO outperforms single-mechanism setups and baseline in average reward and stability.}
    \label{fig:dual-mechanism}
\end{figure}

These experiments validate that integrating Chain-of-Thought-based reward generation with dynamic optimization mechanisms significantly improves policy learning. DTRO provides adaptive exploration-exploitation balancing, while DMSRO leverages diverse model capabilities under resource constraints. Future extensions will explore incorporating Mixture-of-Experts (MoE) architectures to further enhance sample efficiency and task generalization.




% \begin{figure}[!ht]
% \centering
% \includegraphics[width=0.9\linewidth]{temperature_correlation.pdf}
% \caption{Reward–temperature correlation in CartPole (DTRO active)}
% \label{fig:temp_corr}
% \end{figure}

\section{Discussion}
While our method improves reward adaptability and task generalization, limitations persist in model switching costs and reward interpretability. Future efforts may include structured prompting \cite{chen2022, gao2023}, hybrid reward learning \cite{skalse2022defining}, and MoE architecture integration.

\section{Conclusion}
We propose a dual-dynamic optimization framework for CoT-based RL reward generation, incorporating temperature regulation and model selection. Our results demonstrate improved convergence, stability, and reward quality across tasks, laying a foundation for scalable, self-adaptive reward engineering.

\bibliographystyle{IEEEtran} % 或其他你需要的样式
\bibliography{adaptive_cot_reward_rl} % 不需要加.bib扩展名

\end{document}
