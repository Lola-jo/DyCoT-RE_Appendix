\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. 
% If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multirow}

\DeclareRobustCommand*{\IEEEauthorrefmark}[1]{%
    \raisebox{0pt}[0pt][0pt]{\textsuperscript{\footnotesize\ensuremath{#1}}}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{DyCoT‑RE: Chain-of-Thought-Enhanced LLM Reward Engineering with Dual-Dynamic Optimization for Reinforcement Learning}


\author{
\IEEEauthorblockN{1\textsuperscript{st} Xinning Zhu}
\IEEEauthorblockA{\textit{Sino-European School of Technology} \\
\textit{Shanghai University}\\
Shanghai, China \\
zhuxinning@shu.edu.cn}
~\\
\and
\IEEEauthorblockN{2\textsuperscript{nd} Jinxin Du}
\IEEEauthorblockA{\textit{Sino-European School of Technology} \\
\textit{Shanghai University}\\
Shanghai, China \\
jinxin\_du@shu.edu.cn}
~\\
\and
\IEEEauthorblockN{3\textsuperscript{th} Lunde Chen*}
\IEEEauthorblockA{\textit{Sino-European School of Technology} \\
\textit{Shanghai University}\\
Shanghai, China \\
lundechen@shu.edu.cn}
*Corresponding author
}

\maketitle

\begin{abstract}
Designing effective reward functions remains a challenge in applying reinforcement learning to real-world tasks.
This paper proposes DyCoT-RE, a reward engineering framework that integrates Chain-of-Thought (CoT) reasoning with a dual-dynamic 
optimization strategy to automate and enhance reward function design.
The framework uses structured CoT reasoning throughout training to generate and refine interpretable reward code in each iteration.
It further incorporates two dynamic optimization mechanisms: a temperature adjustment strategy that modulates the sampling temperature 
based on policy entropy trends,
and a model switching strategy that allocates language models with different capabilities to produce distinct reward components.
Evaluations on CartPole, BipedalWalker, Ant, and a custom SpaceMining environment show DyCoT‑RE achieves higher average rewards and faster 
convergence compared to human-designed baselines and non‑CoT approaches as well as single-optimization approaches.

   \textcolor{red}{Designing effective reward functions remains a bottleneck for applying reinforcement learning (RL) to complex 
   real‑world tasks. 
   We introduce DyCoT‑RE, a reward engineering framework that leverages Chain‑of‑Thought (CoT) reasoning alongside a dual‑dynamic optimization strategy, 
   incorporating a dynamic temperature regulation optimization and a dynamic model selection routing optimization 
   —which interactively co‑adapt to refine reward functions in real time. 
   Our framework employs CoT prompts to convert high‑level task descriptions into structured, executable reward code, 
   enhancing interpretability and generalization. 
   ??? continuously tunes the LLM sampling temperature based on policy entropy and performance feedback, balancing exploration and stability. 
   Concurrently, ??? fuses local reward metrics with global training outcomes to dynamically select the most effective language model, 
   optimizing sampling efficiency and resource usage. 
   Evaluations on CartPole, BipedalWalker, Ant, and a custom SpaceMining environment show DyCoT‑RE achieves higher average rewards and faster 
   convergence compared to human-designed baselines and non‑CoT approaches as well as single-optimization approaches.}



\end{abstract}

\begin{IEEEkeywords}
Reinforcement learning, reward engineering, large language models, chain-of-thought reasoning, dynamic temperature adjustment, model selection
\end{IEEEkeywords}

\section{Introduction}

Reinforcement learning (RL) has achieved impressive results in a variety of domains,
yet its practical deployment remains limited by the challenge of designing effective reward functions.
Constructing dense and well-shaped rewards often requires significant domain expertise
and extensive environment interaction, creating barriers to scalability and adaptability \cite{sutton1998reinforcement, arora2021survey}.

While carefully designed rewards can accelerate agent learning and improve task performance,
manual reward engineering typically relies on trial-and-error tuning,
which is labor-intensive and often yields suboptimal generalization to new environments or objectives.
As RL applications grow in complexity, there is a pressing need for methods that can automate
reward design while maintaining interpretability and flexibility.

Recent advances in large language models (LLMs) have demonstrated strong reasoning and generalization capabilities \cite{brown2020language, ouyang2022training}.
In particular, Chain-of-Thought (CoT) reasoning enables LLMs to decompose tasks into structured intermediate steps,
enhancing clarity and alignment with desired objectives.
This structured reasoning process can support reward engineering by converting task descriptions
into executable reward functions in a systematic and transparent manner.

However, existing CoT-based reward generation approaches typically use static sampling parameters and fixed model configurations,
which may limit their adaptability during training.
Motivated by recent progress in dynamic temperature adjustment and model selection strategies,
we explore whether integrating adaptive optimization mechanisms into CoT-based reward generation
can improve reward quality, training stability, and sampling efficiency.
Dynamic temperature modulation can adjust exploration levels throughout training,
while model selection strategies can allocate LLMs with specialized capabilities to different reward generation tasks.

In this work, we propose DyCoT-RE, a reward engineering framework that integrates structured CoT reasoning
with dual-dynamic optimization.
DyCoT-RE applies CoT reasoning throughout the training process to iteratively generate and refine reward functions,
while incorporating adaptive temperature adjustment based on policy entropy trends,
and dynamic model selection to assign LLMs with suitable strengths to specific components of reward generation.

We evaluate DyCoT-RE on four standard RL environments—CartPole, MountainCarContinuous, BipedalWalker, and Ant—
as well as a custom SpaceMining environment.
Results show that DyCoT-RE achieves improved average rewards and faster convergence compared to baselines and non-CoT methods.

The remainder of this paper is organized as follows.
Section II reviews related work in reward engineering, LLM-based reward generation, and adaptive optimization.
Section III describes the proposed methodology, including the CoT reward framework, temperature adjustment, and model selection.
Section IV details the experimental setup, and Section V presents results and analysis.
Section VI discusses limitations and future work, with Section VII concluding the paper.

\section{Related Work}

\subsection{Reward Engineering Paradigms}

In RL, the design of effective reward functions directly shapes agent behavior and learning outcomes. Traditional approaches primarily rely on handcrafted reward functions informed by domain expertise. While intuitive, such manual design often struggles to capture complex, dynamic task objectives and is prone to suboptimal or biased formulations, hindering agent performance in real-world scenarios.

To address these limitations, reward shaping was introduced as a formal enhancement strategy. Ng et al. \cite{ng1999policy} demonstrated that potential-based reward shaping preserves optimal policies while enabling accelerated convergence, laying the theoretical foundation for numerous practical implementations. Intrinsic motivation frameworks further advanced this field by encouraging exploration through curiosity-driven signals. Singh et al. \cite{singh2010intrinsically} proposed intrinsic rewards to incentivize novel state visits, later extended by Burda et al. \cite{burda2018exploration}, who empirically validated large-scale curiosity-driven exploration benefits across diverse environments.

Despite these developments, manually designing rewards for complex or evolving tasks remains inefficient and costly. LLMs offer a promising alternative by leveraging their natural language understanding to automate reward generation and optimization. Unlike traditional RL pipelines that require explicit, task-specific reward formulations, LLMs can interpret high-level task descriptions, extract key objectives, and translate them into executable reward functions. This capability facilitates more intuitive alignment with human intentions, reduces engineering overhead, and enhances agent adaptability.

Recent frameworks exemplify this trend. EUREKA \cite{ma2023eureka}, Text2Reward \cite{xie2023text2reward}, and CARD \cite{sun2024large} harness LLMs to automatically generate, verify, and refine reward code from natural language instructions, reducing manual intervention while improving correctness and alignment. Notably, CARD emphasizes the importance of integrated verification loops, eliminating reliance on external human feedback and enhancing system robustness.

Beyond static code generation, feedback-driven optimization approaches have emerged. ReMiss \cite{xie2024jailbreaking} utilizes adversarial prompt generation to identify and mitigate reward misspecification vulnerabilities, enhancing LLM safety and reliability. Self-Play Preference Optimization (SPPO) \cite{wu2024self} employs self-play to uncover Nash-equilibrium strategies that capture complex, non-transitive human preferences, advancing preference learning's applicability in RL. Additionally, PRMBench \cite{song2025prmbench} provides a process-level benchmark to evaluate intermediate reward model outputs along dimensions such as conciseness, rationality, and sensitivity, revealing weaknesses in current models and guiding future improvements.

Overall, LLM-based reward engineering represents a paradigm shift. By integrating natural language reasoning and dynamic feedback optimization, these methods offer scalable, adaptable, and human-aligned reward generation pipelines. As tasks grow in complexity and diversity, leveraging LLMs to bridge the gap between human intent and machine learning objectives will be critical for the next generation of intelligent systems. Continued research is thus needed to maximize the synergy between LLM capabilities and RL frameworks to address emerging real-world challenges.

\subsection{Chain-of-Thought Reasoning Methods}

CoT reasoning has emerged as a powerful paradigm to enhance the reasoning capabilities of LLMs. By generating intermediate reasoning steps, CoT allows models to decompose complex problems into interpretable sub-problems, leading to significant performance gains in tasks requiring multi-step logical inference.

Early studies revealed that even simple prompting strategies, such as adding ``Let's think step by step'' to inputs, can elicit surprisingly strong zero-shot reasoning abilities from LLMs. Kojima et al. \cite{kojima2022large} demonstrated that such zero-shot CoT prompting significantly improves model performance in arithmetic and commonsense reasoning tasks. Building on this, few-shot CoT \cite{liu2022few, wei2022chain} introduced demonstrations of step-wise solutions to guide models in decomposing complex problems, while self-consistency decoding \cite{wang2022self} aggregated multiple sampled reasoning paths to select the most consistent answer, thereby enhancing robustness.

Subsequent works have advanced CoT through architectural and algorithmic innovations. For instance, DeepSeek introduced a reinforcement learning-trained model \cite{deepseek2023r1} achieving an AIME benchmark accuracy improvement from 71.0\% to 86.7\% without supervised fine-tuning, utilizing a ``self-evolution'' mechanism to optimize reasoning trajectories and generation length dynamically. Program-aided language models \cite{gao2023} combine symbolic program execution with CoT for mathematical reasoning, while automatic prompt generation methods, such as zero-shot CoT and semi-automatic CoT prompt optimization \cite{shum2023automatic, pitis2023}, reduce reliance on manual prompt engineering by leveraging data-driven prompt refinement.

Enhancement techniques such as VerifyCoT \cite{ling2023deductive} and Self-Refine \cite{madaan2023} validate and iteratively refine intermediate reasoning steps to improve accuracy and reliability. Problem decomposition methods, including least-to-most prompting \cite{zhou2022least} and successive prompting \cite{dua2022}, guide models from simple to complex sub-tasks to reduce cognitive load and improve solution rates. Knowledge-augmented CoT approaches integrate external knowledge bases to ground reasoning, as demonstrated by Chain-of-Knowledge prompting \cite{wang2023c} and self-prompted CoT for open-domain multi-hop reasoning \cite{wang2023e}. In numerical reasoning, Program-of-Thoughts prompting \cite{chen2022} disentangles computational execution from logical inference to enhance transparency, while LINC \cite{olausson2023} combines neurosymbolic logic proving with CoT for explainable reasoning.

Applications combining CoT with external tools have expanded LLM capabilities in code generation, planning, and multi-step task execution \cite{parisi2022talm, liu2023llm+}. Distilling stepwise reasoning processes reduces inference costs while maintaining high accuracy. Recent frameworks integrate CoT with Monte Carlo Tree Search (MCTS) to systematically explore reasoning pathways \cite{pan2025coat}, and abstraction-based reasoning enhances efficiency in complex mathematical tasks \cite{shao2024deepseekmath}. Moreover, generating concise intermediate outputs has enabled state-of-the-art performance on GSM8K math benchmarks with reduced computation overhead \cite{xu2025chain}.

In summary, CoT reasoning has established itself as a fundamental methodology for enhancing the reasoning capacity, interpretability, and robustness of LLMs. Its diverse technical innovations not only improve performance across a wide range of cognitive tasks but also provide theoretical and practical foundations for developing transparent and trustworthy AI systems.

\subsection{Dynamic Temperature Adjustment and Model Selection}

Dynamic temperature adjustment and model selection have emerged as critical optimization strategies to enhance the adaptability and efficiency of large language model (LLM)-based systems. Temperature, as a sampling hyperparameter, controls the stochasticity of LLM outputs, thereby influencing creativity, coherence, and exploration-exploitation trade-offs.

Recent studies have systematically explored adaptive temperature mechanisms. Zhu et al. \cite{zhu2024hot} proposed adaptive temperature sampling for code generation, dynamically adjusting temperatures based on task complexity and model uncertainty to improve generation quality. Cecere et al. \cite{cecere2025monte} introduced Monte Carlo Temperature, a robust sampling strategy for uncertainty quantification, enhancing LLM reliability under distribution shifts. Similarly, Zhang et al. \cite{zhang2024edt} developed Entropy-based Dynamic Temperature (EDT) sampling to regulate model entropy and output diversity in natural language generation. Peeperkorn et al. \cite{peeperkorn2024temperature} examined temperature's role in creativity modulation, while Evstafev \cite{evstafev2025paradox} highlighted potential limitations in creativity gains versus computational decoupling in structured data generation. Nguyen et al. \cite{nguyen2024turning} proposed min-p sampling, adjusting temperature to balance creativity and coherence, achieving state-of-the-art performance in narrative generation.

Model selection research, on the other hand, focuses on choosing optimal model configurations or expert modules to maximize task performance within computational constraints. Switch Transformers \cite{fedus2022switch} introduced sparse activation for trillion-parameter models, enabling efficient expert selection. ME-Switch presented a memory-efficient switching framework for dynamic expert allocation in LLMs. In continual learning, switching mechanisms facilitate instruction tuning to adapt models to evolving task distributions, as explored in Switching for Continual Instruction Tuning. GLaM \cite{du2022glam} leveraged Mixture-of-Experts (MoE) architectures to scale model capacity dynamically. QLoRA and DyLoRA proposed quantization-aware and low-rank adaptation methods for efficient parameter tuning and rapid model switching across tasks. Furthermore, LoraHub introduced dynamic LoRA composition to enhance cross-task generalization, while Self-Expansion with Mixture of Adapters \cite{wang2025self} enabled continual learning via adaptive expert composition.

Despite these advances, integrating dynamic temperature regulation and model selection within a unified CoT-driven reward engineering framework remains underexplored. Existing temperature adaptation methods primarily focus on text generation diversity and confidence calibration, whereas model selection research emphasizes computational efficiency and task specialization. Our work addresses this gap by combining entropy- and reward-feedback-based temperature adjustment with local-global performance-based model routing to enhance RL reward generation's adaptability, stability, and sample efficiency. This approach builds upon foundational theories in temperature scaling and expert selection, extending them to the domain of automated, interpretable reward engineering for reinforcement learning.

\subsection{Differentiation and Contribution of This Work}

Despite significant advances in reward engineering, CoT reasoning, and adaptive optimization techniques, existing approaches remain fragmented and task-specific. Traditional reward shaping methods rely heavily on expert-designed heuristics, limiting their adaptability to unseen tasks and dynamic environments. While recent frameworks such as EUREKA and Text2Reward leverage LLMs to automate reward design, they generally operate under static generation paradigms without runtime optimization. This limits their ability to accommodate dynamic training processes in RL, potentially resulting in suboptimal exploration-exploitation balance or brittle convergence.

CoT reasoning has emerged as a powerful mechanism for enhancing the interpretability and logical consistency of LLM outputs, demonstrating strong performance in mathematical and logical reasoning tasks. However, its application in RL reward engineering remains underexplored, with prior work rarely integrating CoT reasoning into automated reward pipelines. Additionally, while adaptive temperature adjustment techniques have been shown to improve sampling diversity and stability, and dynamic model selection frameworks such as Switch Transformers and GLaM improve computational efficiency and scalability in LLMs, there exists a lack of research on jointly integrating these dynamic optimization strategies within CoT-based reward generation systems.

To address these limitations, this work introduces a unified framework that combines CoT-enhanced reward generation with dual-dynamic optimization mechanisms. Specifically, it presents a structured CoT-based reward generation approach that translates natural language task descriptions into interpretable and executable reward functions, enhancing generalization and sample efficiency. Building upon this, the proposed Dynamic Temperature Regulation Optimization (DTRO) module adjusts LLM sampling temperature in real time based on policy entropy and performance feedback, effectively balancing exploration and exploitation during training. Furthermore, the Dynamic Model Selection Routing Optimization (DMSRO) module integrates local reward evaluation with global performance assessments to dynamically switch among multiple LLMs, improving reward generation quality while optimizing computational resource usage.

The effectiveness of this framework is systematically validated across four standard RL benchmarks—CartPole, MountainCarContinuous, BipedalWalker, and Ant—as well as a custom-designed SpaceMining environment. Results demonstrate superior performance in terms of average reward, convergence speed, resource consumption, and robustness compared to both baseline methods and single optimization variants. Collectively, this study establishes a novel paradigm for automated, interpretable, and adaptive reward engineering by synergistically combining CoT reasoning with dynamic optimization mechanisms, advancing the capability of RL systems in tackling complex, dynamic tasks.




\section{Methodology}
\subsection{Architecture Overview}
The proposed framework combines evolutionary search with dynamic reward optimization, as illustrated in Fig.~\ref{fig:architecture} and Fig.~\ref{fig:evolution}. The system processes natural language inputs (e.g., "Design a reward function for stable bipedal robot walking") through a dual-path mechanism:

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{./Figures/architecture.png}
\caption{Reward function design framework. Top: Natural language task specification and environment interface. Middle: Chain-of-Thought reasoning (left) and dynamic optimization modules (right). Bottom: Executable output and monitoring system.}
\label{fig:architecture}
\end{figure}

\subsection{Chain-of-Thought Generation}
The left branch in Fig.~\ref{fig:architecture} demonstrates the three-stage decomposition process:
\begin{equation}
\text{CoT}(d) \rightarrow \begin{cases}
\text{Decompose goal} & \text{(e.g., balance, forward motion)} \\
\text{Identify key states} & \text{(e.g., torso angle $\theta$)} \\
\text{Mathematical modeling} & \text{(e.g., $r_t = w_1\cos\theta + w_2v_x$)}
\end{cases}
\end{equation}

\subsection{Dynamic Optimization}
The evolutionary loop in Fig.~\ref{fig:evolution} operates through:
\begin{equation}
P_{t+1} = \underbrace{\text{Select}(P_t, k=0.2)}_{\text{Elite selection}} \oplus \underbrace{\text{Mutate}(\theta, T)}_{\substack{\text{Temperature-}\\ \text{controlled}}}
\end{equation}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{./Figures/evolution.png}
\caption{Evolutionary search loop for reward optimization. The process iteratively refines reward functions through parallel RL training, with fitness evaluation driving selection and mutation.}
\label{fig:evolution}
\end{figure}

The DTRO module regulates exploration-exploitation balance via entropy-aware temperature adjustment:
\begin{equation}
\Delta T = \beta \frac{\partial H}{\partial t} \cdot \mathbb{I}(\sigma_R > \tau)
\end{equation}

where $\mathbb{I}(\cdot)$ is the indicator function. The DMSRO component, shown in the right branch of Fig.~\ref{fig:architecture}, optimizes model selection through:
\begin{equation}
m^* = \mathop{\mathrm{arg\,max}}\limits_{m \in \mathcal{M}} \left( \alpha \cdot \text{Score}(m) + (1-\alpha)\cdot \text{Efficiency}(m) \right)
\end{equation}

\subsection{Integration Mechanism}
The two diagrams collectively demonstrate how initial CoT-generated rewards evolve through:
\begin{itemize}
\item Continuous refinement via the evolutionary loop (Fig.~\ref{fig:evolution})
\item Real-time adaptation through dynamic modules (Fig.~\ref{fig:architecture})
\end{itemize}

The fitness function $F=0.4S+0.3C+0.3E$ in Fig.~\ref{fig:evolution} ensures balanced optimization of stability ($S$), completion ($C$), and efficiency ($E$).

The proposed framework models reward generation as a function of task description, temperature, and model:
\begin{equation}
R(s,a,t) = \Phi(d, T(t), m(t))
\end{equation}
where $\Phi$ denotes the CoT-based LLM generation process.

\subsection{Dynamic Temperature Regulation (DTRO)}
DTRO regulates the LLM sampling temperature based on policy entropy $H_t$ and confidence $C_t$. The update rule is:
\begin{equation}
\Delta T_t = \beta \Delta T_{t-1} + (1-\beta) \left[\alpha_1 \tanh\left(\frac{H_t - \bar{H}}{\sigma_H}\right) + \alpha_2(C_t - \theta_c)\right]
\end{equation}
This mechanism adapts the creativity and determinism of LLM outputs according to policy performance.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{./Figures/reward_episode_curve.png}
    \caption{Average reward over training episodes in five environments. The full system (blue) shows improved convergence and final reward compared to the baseline (orange).}
    \label{fig:reward_curve}
\end{figure*}

\subsection{Dynamic Model Selection (DMSRO)}
Let $\mathcal{M}$ be a set of candidate models. The selection score for model $m$ is:
\begin{equation}
p_{\text{fused}}(m) = (1-\gamma) \cdot p_{\text{local}}(m) + \gamma \cdot p_{\text{hist}}(m)
\end{equation}
An $\epsilon$-greedy strategy is applied to explore new models while exploiting high-performing ones.

\section{Experiments}
\label{sec:exp}



This section presents the experimental evaluation of the proposed Chain-of-Thought reward generation framework with two adaptive optimization mechanisms: Dynamic Temperature Regulation (DTRO) and Dynamic Model Selection for Reward Optimization (DMSRO). Experiments are conducted in five representative environments to assess performance improvement, training efficiency, and system stability.

The experiments include the following environments: CartPole (control task), MountainCar (sparse reward task), BipedalWalker (locomotion task), Ant (high-dimensional locomotion task), and SpaceMining (a custom-designed single-agent mining environment). For the baseline comparison, standard Gymnasium environment runs with equivalent hyperparameters are used. In SpaceMining, due to the environment being newly created, baseline is approximated by evaluating the environment with standard random policies and heuristic reward shaping to provide approximate reference values. This limitation will be addressed in future work by integrating alternative learning-based baselines or expert demonstrations.

\subsection{Overall Reward Performance}

Figure \ref{fig:reward_curve} shows the average reward curves over training episodes for the full system compared to the baseline in each environment. The horizontal axis represents training episodes, while the vertical axis shows the average reward achieved by the agent. Across all environments, the proposed CoT framework with DTRO and DMSRO demonstrates significantly faster convergence speed and higher final reward performance. In CartPole and MountainCar, the method achieves near-optimal performance within fewer episodes. For BipedalWalker and Ant, which are high-dimensional control tasks, reward increases more steadily with lower variance compared to the baseline. In SpaceMining, despite lacking a formal baseline, the method shows effective reward shaping, demonstrating the adaptability of CoT-based reward generation to custom task domains.



Table~\ref{tab:performance-comparison} presents the quantitative results, reporting average reward, maximum reward, standard deviation, and average convergence episodes. The proposed framework achieves significant improvements, particularly in the Ant and SpaceMining environments, highlighting its scalability in high-dimensional and custom task settings.

\begin{table}[ht]
\caption{Performance comparison across environments}
\label{tab:performance-comparison}
\centering
\begin{tabular}{lcccc}
\hline
Environment & Avg. Reward & Max Reward & Std. Dev & Conver. Ep. \\
\hline
CartPole & 195.2 & 200.0 & 4.3 & 110 \\
MountainCar & -110.4 & -85.2 & 12.8 & 350 \\
BipedalWalker & 312.4 & 340.1 & 15.5 & 420 \\
Ant & 2867.5 & 3150.0 & 185.7 & 920 \\
SpaceMining & 218.7 & 240.3 & 21.1 & 1350 \\
\hline
\end{tabular}
\end{table}

\subsection{Temperature-Entropy-Reward Correlation Analysis}

Figure \ref{fig:temp_entropy_reward} illustrates the three-dimensional heatmap of temperature, policy entropy, and average reward under the DTRO mechanism. The horizontal axis represents the temperature values sampled during training, the vertical axis shows normalized policy entropy, and the color bar indicates the corresponding average reward achieved. The figure reveals that under dynamic temperature adjustment, the system maintains a balance between exploration and exploitation by stabilizing entropy near mid-range values (0.4-0.6) while progressively lowering temperature as the policy converges. This dynamic adjustment yields higher rewards in regions of moderate entropy, validating the effectiveness of entropy-aware temperature control.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.45\textwidth]{./Figures/temp_entropy_reward_heatmap.png}
  \caption{Temperature-entropy-reward correlation heatmap under DTRO. X-axis: Temperature ($T$), Y-axis: normalized policy entropy ($H$), Color: average reward.}
  \label{fig:temp_entropy_reward}
\end{figure}

Furthermore, ablation experiments comparing static temperature to DTRO-adjusted temperature demonstrate that dynamic regulation reduces reward variance by an average of 17.2\% and improves convergence speed by 13.5\%.

\subsection{Dynamic Model Selection Analysis}

Figure \ref{fig:model_switch_log} presents the model switching log visualization under the DMSRO mechanism. The horizontal axis represents training timesteps, while different colors indicate the models selected at each step. The vertical stacked area shows either the reward level (scaled) or switching frequency over time. The plot demonstrates that during early training, the framework frequently switches between diverse models to enhance exploration and diversity in reward generation. In later stages, model selection stabilizes, with the framework consistently choosing models yielding the highest rewards for efficient policy refinement. This adaptive switching behavior confirms the DMSRO mechanism's ability to balance computational efficiency and reward quality by selecting models dynamically based on local performance and historical trends.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{./Figures/model_switch_log.png}
    \caption{DMSRO model switching log. X-axis: timestep, color: selected model (LLaMA-3, Qwen-2.5, DeepSeek-R1), line: reward progression. The system dynamically allocates models to balance performance and resource usage.}
    \label{fig:model_switch_log}
\end{figure}


\subsection{Joint System Performance}

Table \ref{tab:overall_perf} summarizes the joint system performance across environments. Metrics include average reward, convergence episode (defined as reaching 90\% of maximum reward), and reward variance. Results indicate that the full framework integrating DTRO and DMSRO consistently outperforms configurations with DTRO only, DMSRO only, or static baseline. This demonstrates the synergistic effect of temperature regulation and model selection in improving both learning efficiency and final policy robustness.

\begin{table}[ht]
\centering
\caption{Joint system performance comparison across environments}
\label{tab:overall_perf}
\begin{tabular}{l|cccc}
\hline
\textbf{Env} & \textbf{Reward (↑)} & \textbf{Conv.(↓)} & \textbf{Var. (↓)} & \textbf{Config} \\
\hline
CartPole & 210.7 & 75 & 8.5 & DTRO + DMSRO \\
MountainCar & 93.5 & 140 & 12.3 & DTRO + DMSRO \\
BipedalWalker & 312.4 & 480 & 38.6 & DTRO + DMSRO \\
Ant & 2750.9 & 980 & 201.4 & DTRO + DMSRO \\
SpaceMining & 134.2 & 560 & 45.8 & DTRO + DMSRO \\
\hline
\end{tabular}
\end{table}

Overall, the experimental results validate the effectiveness of the proposed Chain-of-Thought reward generation framework with integrated adaptive optimization mechanisms. The joint system exhibits superior learning performance, faster convergence, and greater stability compared to baseline or partial configurations, establishing a promising foundation for scalable automatic reward engineering in complex reinforcement learning tasks.

Finally, experiments combining DTRO and DMSRO confirm their synergistic benefit. Figure~\ref{fig:dual-mechanism} illustrates the comparative performance of four system configurations: baseline, DTRO only, DMSRO only, and the full system. The joint optimization achieves the highest reward with the lowest training variance, demonstrating the proposed framework’s effectiveness in adaptive reward engineering.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{./Figures/dual_mechanism_comparison.png}
    \caption{Performance comparison across system configurations. Joint DTRO+DMSRO outperforms single-mechanism setups and baseline in average reward and stability.}
    \label{fig:dual-mechanism}
\end{figure}

These experiments validate that integrating Chain-of-Thought-based reward generation with dynamic optimization mechanisms significantly improves policy learning. DTRO provides adaptive exploration-exploitation balancing, while DMSRO leverages diverse model capabilities under resource constraints. Future extensions will explore incorporating Mixture-of-Experts (MoE) architectures to further enhance sample efficiency and task generalization.




% \begin{figure}[!ht]
% \centering
% \includegraphics[width=0.9\linewidth]{temperature_correlation.pdf}
% \caption{Reward–temperature correlation in CartPole (DTRO active)}
% \label{fig:temp_corr}
% \end{figure}

\section{Discussion}
While our method improves reward adaptability and task generalization, limitations persist in model switching costs and reward interpretability. Future efforts may include structured prompting \cite{chen2022, gao2023}, hybrid reward learning \cite{skalse2022defining}, and MoE architecture integration.

\section{Conclusion}
We propose a dual-dynamic optimization framework for CoT-based RL reward generation, incorporating temperature regulation and model selection. Our results demonstrate improved convergence, stability, and reward quality across tasks, laying a foundation for scalable, self-adaptive reward engineering.

\bibliographystyle{IEEEtran} % 或其他你需要的样式
\bibliography{adaptive_cot_reward_rl} % 不需要加.bib扩展名

\end{document}
